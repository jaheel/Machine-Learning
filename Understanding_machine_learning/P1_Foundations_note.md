# P1 Foundations

## A Gentle Start

### A Formal Model — The Statistical Learning Framework

* The learner's input

  > * Domain set: An arbitrary set, $X$. This is the set of objects that we may wish to label.
  > * Label set: usually {0,1} or {-1,+1}. Let $y$ denote our set of possible labels.
  > * Training data: $S=((x_1,y_1)...(x_m,y_m))$ is a finite sequence of pairs in $X \times y$

* The learner's output

  > * a *prediction rule*: $h: X \rightarrow y$ (a *predictor*, *a hypothesis*, or a *classifier*)
  > * the notation $A(S)$: denote the hypothesis that a learning algorithm, $A$, returns upon receiving the training sequence $S$.

* A simple data-generation model

  > * $D$ : denote that probability distribution over $X$
  > * $ f: X \rightarrow y$ , and that $y_i = f(x_i)$ for all $i$ : "correct" labeling function

* Measures of success

  > * *error of a classifier*: the probability that it does not predict the correct label on a random data point generated by the aforementioned underlying distribution.
  >   $$
  >   L_{D,f}(h) \xlongequal {def} \Rho _{x \backsim D}[h(x) \neq  f(x)] \xlongequal {def} D(\big \{ x:h(x) \neq f(x) \big \} )
  >   $$





### Empirical Risk Minimization(ERM)

A learning algorithm receives as input a training set $S$, sampled from an unknown distribution $D$ and labeled by some target function $f$, and should output a predictor $h_S: X \rightarrow y$ . 

The goal of the algorithm is to find $h_S$ that minimizes the error with respect to the unknown $D$ and $f$.
$$
L_S(h) \xlongequal{def} \frac{| \{ i \in[m]:h(x_i) \neq y_i \} |}{m}
$$
​		where $[m] = {1,...,m}$.



### Overfitting 

A predictor whose performance on the training set is excellent, yet its performance on the true "world" is very poor.



### Empirical Risk Minimization with Inductive Bias

*  *a hypothesis class* : $H$

* Each $h \in H$ is a function mapping from $X$ to $y$.

* For a given class $H$, and a training sample, $S$, the $ERM_H$ learner uses the $ERM$ rule to choose a predictor $h \in H$, with the lowest possible error over $S$.
  $$
  ERM_H(S) \in \mathop {argmin}_{h \in H} L_S(h)
  $$
  



* *inductive bias* : By restricting the learner to choosing a predictor from $H$, we *bias* it toward a particular set of predictors.





### Probably Approximately Correct(PAC) learning

The preceeding corollary tells us that for a sufficiently large $m$, the $ERM_H$ rule over a finite hypothesis class will be *probably*(with confidence $1-\delta$) *approximately correct*.



## A Formal Learning Model

### PAC learning

For a finite hypothesis class, if the ERM rule with respect to that class is applied on a sufficiently large training sample ( whose size is independent of the underlying distribution or labeling function) then the output hypothesis will be probably approximately correct.



Every finite hypothesis class is PAC learnable with sample complexity:
$$
m_H(\varepsilon , \delta) \le \lceil \frac{log(|H|/ \delta)}{\varepsilon} \rceil
$$






### A More General Learning Model

Consider generalizations in two aspects:

1. Removing the Realizability Assumption

2. Learning Problems beyond Binary Classification

   

* The empirical and the True Error Revised

  > For a probability distribution, $D$, over $X \times y$ , one can measure how likely $h$ is to make an error when labeled points are randomly drawn according to $D$. We redefine the true error( or risk) of a prediction rule $h$ to be
  > $$
  > L_D(h) \xlongequal{def} P_{(x,y) \backsim D}[h(x) \ne y] \xlongequal{def} D(\{ (x,y) : h(x) \ne y \})
  > $$
  > We would like to find a predictor, $h$, for which that error will be minimized.





* The Scope of Learning Problems Modeled

  > * Multiclass Classification
  >
  >   > $y$ will be some large finite set
  >
  > * Regression
  >
  >   > To find some simple *pattern* in the data — a functional relationship between the $X$ and $y$ components of the data.
  >
  >   $$
  >   L_D(h) \xlongequal {def} \mathbb {E}_{(x,y) \backsim D} (h(x)-y)^2
  >   $$



Loss Function:

* 0-1 loss: 
  $$
  \ell_{0-1}(h,(x,y)) \xlongequal{def} 
  \begin{cases}
  0 & \text{if } h(x) = y \\
  1 & \text{if } h(x) \neq y
  \end{cases}
  $$
  This loss function is used in binary or multiclass classification problems.

* Square Loss:
  $$
  \ell_{sq}(h,(x,y)) \xlongequal{def} (h(x)-y)^2
  $$
  This loss function is used in regression problems.







## Learning via Uniform Convergence

learning URL: https://www.cs.umb.edu/~dsim/cs671-17/S5-UNIF.pdf

### Uniform Convergence Is Sufficient for Learnability

* $\epsilon$-representative sample

  > A training set S is called $\epsilon$-representative (w.r.t domain $Z$, hypothesis class $H$, loss function $\ell$, and distribution $D$) if
  > $$
  > \forall {h \in H}, |L_S(h)-L_D(h)| \le \epsilon
  > $$

* Uniform Convergence

  > We say that a hypothesis class $H$ has the *uniform convergence property* (w.r.t a domain $Z$ and a loss function $\ell$) if there exists a function $m_H^{UC}:(0,1)^2 \rightarrow \mathbb{N}$ such that for every $\epsilon,\delta \in (0,1)$ and for every probability distribution $D$ over $Z$, if $S$ is a sample of $m \ge m_H^{UC}(\epsilon,\delta)$ examples drawn i.i.d. according to $D$, then, with probability of at least $1-\delta, S$ is $\epsilon$-representative. 



## The Bias-Complexity Tradeoff

### The No-Free-Lunch Theorem

Prove that there is no universal learner.



* No-Free-Lunch

  > Let A be any learning algorithm for the task of binary classification with respect to the 0-1 loss over a domain $X$. Let $m$ be any number smaller than $|X|/2$, representing a training set size. Then, there exists a distribution $D$ over $X \times {0,1}$ such that:
  >
  > 1. There exists a function $f: X \rightarrow {0,1}$ with $L_D(f) =0$.
  > 2. With probability of at least 1/7 over the choice of $S \backsim D^m$ we have that $L_D(A(S)) \ge 1/8$



### Error Decomposition

$$
L_D(h_S) = \epsilon_{app} + \epsilon_{est} \quad where:\epsilon_{app} = \min_{h \in H} L_D(h), \quad \epsilon_{est} = L_D(h_S)-\epsilon_{app}
$$

* The Approximation Error

  > * the minimum risk achievable by a predictor in the hypothesis class.
  >
  > * The approximation error does not depend on the sample size and is determined by the hypothesis class chosen.
  >
  > * Enlarging the hypothesis class can decrease the approximation error.

* The Estimation Error

  > * the difference between the approximation error and the error achieved by the ERM predictor.



Since our goal is to minimize the total risk, we face a tradeoff, called the *bias-complexity tradeoff*.

> 1. choosing $H$ to be a very rich class decreases the approximation error but at the same time might increase the estimation error.(as a rich $H$ might lead to *overfitting*)
> 2. choosing $H$ to be a very small set reduces the estimation error but might increase the approximation error or, in other words, might lead to *underfitting*.
> 3. a great choice for $H$ is the class that contains only one classifier - the Bayes optimal classifier.



**Learning theory studies how rich we can make $H$ while still maintaining reasonable estimation error.**



## The VC-Dimension

learning URL: https://tangshusen.me/2018/12/09/vc-dimension/



VC-dimension

> The VC-dimension of a hypothesis class $H$, denoted VCdim($H$), is the maximal size of a set $C \subset X$ that can be shattered by $H$. If $H$ can shatter sets of arbitrarily large size we say that $H$ has infinite VC-dimension.



The Fundamental Theorem of Statistical Learning

> Let $H$ be a hypothesis class of functions from a domain $X$ to $\{0,1\}$ and let the loss function be the 0-1 loss. Then , the following are equivalent:
>
> 1. $H$ has the uniform convergence property.
> 2. Any ERM rule is a successful agnostic PAC learner for $H$.
> 3. $H$ is agnostic PAC learnable.
> 4. $H$ is PAC learnable.
> 5. Any ERM rule is a successful PAC learner for $H$.
> 6. $H$ has a finite VC-dimension.