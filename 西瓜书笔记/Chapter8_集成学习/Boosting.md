# Boosting

原理：“三个臭皮匠顶个诸葛亮”



理论前置：

1. PAC学习框架中

   > 强可学习：一个概念（一个类），如果存在一个多项式的学习算法能够学习它，并且正确率很高。
   >
   > 弱可学习：一个概念（一个类），如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随即猜测略好。

2. Schapire证明

   > 强可学习 与 弱可学习 是等价的。（在PAC框架下，一个概念是强可学习的充要条件是这个概念是弱可学习的。



问题：

已经发现了“弱学习算法”，那么能否将它提升(boost)为“强学习算法”？



提升方法：

​		从弱学习算法出发，反复学习，得到一系列弱分类器（又称为 基本分类器），然后组合这些弱分类器，构成一个强分类器。



对提升方法的两个问题：

1. 每一轮如何改变训练数据的权值或概率分布
2. 如何将弱分类器组合成一个强分类器



## AdaBoost

对问题的解答：

1. 提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。

2. 弱分类器的组合（加权多数表决）

   > 加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用
   >
   > 减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用



